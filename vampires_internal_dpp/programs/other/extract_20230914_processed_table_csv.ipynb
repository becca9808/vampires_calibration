{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Numpy Cubes of Double Diff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the resulting arrays:\n",
      "double_diffs_20230914: (16, 8, 5)\n",
      "double_sums_20230914: (16, 8, 5)\n",
      "double_diff_stds_20230914: (16, 8, 5)\n",
      "double_sum_stds_20230914: (16, 8, 5)\n",
      "-0.12775140267587398\n",
      "-0.04172474535301956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path = \"../../data/20230914/20230914_processed_table.csv\"\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Define the unique values for each axis\n",
    "wavelengths = [\"625-50\", \"675-50\", \"725-50\", \"750-50\", \"775-50\"]\n",
    "HWP_angs = data[\"RET-POS1\"].unique()\n",
    "IMR_angs = data[\"D_IMRANG\"].unique()\n",
    "\n",
    "# Initialize numpy arrays to store the median values, starting with NaNs\n",
    "double_diffs_20230914 = np.full([len(HWP_angs), len(IMR_angs), len(wavelengths)], np.nan)\n",
    "double_sums_20230914 = np.full([len(HWP_angs), len(IMR_angs), len(wavelengths)], np.nan)\n",
    "double_diff_stds_20230914 = np.full([len(HWP_angs), len(IMR_angs), len(wavelengths)], np.nan)\n",
    "double_sum_stds_20230914 = np.full([len(HWP_angs), len(IMR_angs), len(wavelengths)], np.nan)\n",
    "\n",
    "# Calculate median values and store them in the arrays, or NaN if no rows match the mask\n",
    "for i, HWP_ang in enumerate(HWP_angs):\n",
    "    for j, IMR_ang in enumerate(IMR_angs):\n",
    "        for k, wavelength in enumerate(wavelengths):\n",
    "            mask_A = (data[\"RET-POS1\"] == HWP_ang) & (data[\"D_IMRANG\"] == IMR_ang) & (data[\"FILTER01\"] == wavelength) & (data[\"U_FLC\"] == \"A\")\n",
    "            mask_B = (data[\"RET-POS1\"] == HWP_ang) & (data[\"D_IMRANG\"] == IMR_ang) & (data[\"FILTER01\"] == wavelength) & (data[\"U_FLC\"] == \"B\")\n",
    "            \n",
    "            # Check if there are any rows matching the mask\n",
    "            if mask_A.any() and mask_B.any():\n",
    "                # For double sums and diff numerators/denominators - based on Boris' AO188 cal presentation\n",
    "                unnormalized_double_diff = data[mask_A][\"diff\"].median() - data[mask_B][\"diff\"].median()\n",
    "                unnormalized_double_sum = data[mask_A][\"diff\"].median() + data[mask_B][\"diff\"].median()\n",
    "                total_sum = data[mask_A][\"sum\"].median() + data[mask_B][\"sum\"].median()\n",
    "                \n",
    "                # For double sums and diff final values - based on Boris' AO188 cal presentation\n",
    "                normalized_double_diff = unnormalized_double_diff / total_sum\n",
    "                normalized_double_sum = unnormalized_double_sum / total_sum\n",
    "\n",
    "                # For double diff and sum stds - based on regular error propagation\n",
    "                unnormalized_double_diff_std = \\\n",
    "                    np.sqrt(data[mask_A][\"diff_std\"].median() ** 2 + data[mask_B][\"diff_std\"].median() ** 2)\n",
    "                unnormalized_double_sum_std = unnormalized_double_diff_std\n",
    "                total_sum_std = \\\n",
    "                    np.sqrt(data[mask_A][\"sum_std\"].median() ** 2 + data[mask_B][\"sum_std\"].median() ** 2)\n",
    "\n",
    "                # Saving to numpy cubes\n",
    "                double_diffs_20230914[i, j, k] = normalized_double_diff\n",
    "                double_sums_20230914[i, j, k] = normalized_double_sum\n",
    "                double_diff_stds_20230914[i, j, k] = \\\n",
    "                    np.sqrt((unnormalized_double_diff_std / unnormalized_double_diff) ** 2  + (total_sum_std / total_sum) ** 2) * normalized_double_diff\n",
    "                double_sum_stds_20230914[i, j, k] = \\\n",
    "                    np.sqrt((unnormalized_double_sum_std / unnormalized_double_sum) ** 2  + (total_sum_std / total_sum) ** 2) * normalized_double_sum\n",
    "\n",
    "# Get the directory of the CSV file\n",
    "output_dir = os.path.dirname(csv_file_path)\n",
    "\n",
    "# Save the numpy arrays to files in the same directory as the CSV file\n",
    "np.save(os.path.join(output_dir, 'double_diffs_20230914.npy'), double_diffs_20230914)\n",
    "np.save(os.path.join(output_dir, 'double_sums_20230914.npy'), double_sums_20230914)\n",
    "np.save(os.path.join(output_dir, 'double_diff_stds_20230914.npy'), double_diff_stds_20230914)\n",
    "np.save(os.path.join(output_dir, 'double_sum_stds_20230914.npy'), double_sum_stds_20230914)\n",
    "\n",
    "# Output the shapes of the resulting arrays\n",
    "print(\"Shapes of the resulting arrays:\")\n",
    "print(\"double_diffs_20230914:\", double_diffs_20230914.shape)\n",
    "print(\"double_sums_20230914:\", double_sums_20230914.shape)\n",
    "print(\"double_diff_stds_20230914:\", double_diff_stds_20230914.shape)\n",
    "print(\"double_sum_stds_20230914:\", double_sum_stds_20230914.shape)\n",
    "\n",
    "print(double_sums_20230914[0, 0, 0])\n",
    "print(double_sum_stds_20230914[0, 0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Scipy Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Small value to remove error bars\n",
    "log_f = -10\n",
    "\n",
    "# Defining model angles\n",
    "model_angles = np.linspace(0, 90, 100)\n",
    "\n",
    "# List to store all the solutions \n",
    "solns = []\n",
    "\n",
    "# Initial values\n",
    "theta_pol = -3.7768300814382085\n",
    "delta_m3 = 0 # (waves) - assumed to be a perfect mirror for now\n",
    "epsilon_m3 = 0  # Using the M3 diattenuation from :all_unpolarized_standards_matrix_inversion_m3_diatttenuation\"\n",
    "offset_m3 = 0  # NOTE: Made this zero too for testing purposes\n",
    "delta_HWP = 0.451  # Add your actual delta_HWP value\n",
    "offset_HWP = -2.642  # Add your actual offset_HWP value\n",
    "delta_derot = 0.32  # Add your actual delta_derot value\n",
    "offset_derot = -0.011  # Add your actual offset_derot value\n",
    "delta_opts = -0.163  # Add your actual delta_opts value\n",
    "epsilon_opts = 0.036  # Add your actual epsilon_opts value\n",
    "rot_opts = -7.151  # Add your actual rot_opts value\n",
    "delta_FLC = 0.302  # Add your actual delta_FLC value\n",
    "rot_FLC = 0.256  # Add your actual rot_FLC value\n",
    "em_gain = 1 / 1.1342789620513443 # From looking at unpol standards fluxes\n",
    "\n",
    "# Initial guess based on the parameters you want to minimize\n",
    "initial_guess = np.array([theta_pol, delta_HWP, offset_HWP, delta_derot, offset_derot, \n",
    "    delta_opts, epsilon_opts, rot_opts, delta_FLC, rot_FLC])\n",
    "\n",
    "all_params = [delta_m3, epsilon_m3, offset_m3, delta_HWP, offset_HWP, \n",
    "    delta_derot, offset_derot, delta_opts, epsilon_opts, rot_opts, delta_FLC, \n",
    "    rot_FLC, em_gain]\n",
    "\n",
    "# Fixed parameters not included in the fitting process\n",
    "fixed_params = [delta_m3, epsilon_m3, offset_m3, em_gain]\n",
    "\n",
    "# Define the bounds for the parameters (excluding em_gain)\n",
    "bounds = [\n",
    "    (-5, 5),  # theta_pol\n",
    "    (0, 0.5),  # delta_HWP\n",
    "    (-5, 5),  # offset_HWP\n",
    "    (0, 0.5),  # delta_derot\n",
    "    (-5, 5),  # offset_derot\n",
    "    (-0.5, 0.5),  # delta_opts\n",
    "    (0, 0.1),  # epsilon_opts\n",
    "    (-90, 90),  # rot_opts\n",
    "    (0, 0.5),  # delta_FLC\n",
    "    (-90, 90)  # rot_FLC\n",
    "]\n",
    "\n",
    "# Defining the negative log-likelihood function\n",
    "def nll(params, model, HWP_angs, IMR_angs, data, stds):\n",
    "    theta_pol, delta_HWP, offset_HWP, delta_derot, offset_derot, delta_opts, epsilon_opts, rot_opts, delta_FLC, rot_FLC = params\n",
    "    delta_m3, epsilon_m3, offset_m3, em_gain = fixed_params\n",
    "    all_params = [delta_m3, epsilon_m3, offset_m3, delta_HWP, offset_HWP, \n",
    "        delta_derot, offset_derot, delta_opts, epsilon_opts, rot_opts, delta_FLC, \n",
    "        rot_FLC, em_gain]\n",
    "    this_model = instrument_matrices.internal_calibration_mueller_matrix(theta_pol, model, all_params, HWP_angs, IMR_angs)\n",
    "    residuals = this_model - data\n",
    "    \n",
    "    # Debug print shapes\n",
    "    # print(\"Model shape:\", this_model.shape)\n",
    "    # print(\"Data shape:\", data.shape)\n",
    "    # print(\"Stds shape:\", stds.shape)\n",
    "    \n",
    "    likelihood = np.sum((residuals / stds) ** 2)\n",
    "    return likelihood\n",
    "\n",
    "# Initialize variables for the iterative minimization process\n",
    "counter = 0\n",
    "initial_likelihood = 100\n",
    "post_likelihood = 90\n",
    "\n",
    "# Starting off with the initial guess\n",
    "model = instrument_matrices.full_system_mueller_matrix\n",
    "initial_model = instrument_matrices.internal_calibration_mueller_matrix(initial_guess[0], model, all_params, HWP_angs, IMR_angs)\n",
    "\n",
    "while post_likelihood < initial_likelihood:\n",
    "    counter += 1\n",
    "\n",
    "    initial_likelihood = post_likelihood\n",
    "\n",
    "    # Calculate the initial model and residuals\n",
    "    initial_model = instrument_matrices.internal_calibration_mueller_matrix(initial_guess[0], model, all_params, HWP_angs, IMR_angs)\n",
    "    initial_residuals = initial_model - reshaped_data\n",
    "\n",
    "    initial_likelihood = np.sum((initial_residuals / reshaped_stds) ** 2)\n",
    "\n",
    "    print(\"Initial Likelihood: \" + str(initial_likelihood))\n",
    "\n",
    "    # Minimize the negative log-likelihood\n",
    "    minimize_args = (model, HWP_angs, IMR_angs, reshaped_data, reshaped_stds)\n",
    "    soln = minimize(nll, initial_guess, args=minimize_args, bounds=bounds, method=\"Nelder-Mead\")\n",
    "\n",
    "    # Save the solution\n",
    "    solns.append(soln)\n",
    "\n",
    "    # Recalculate the likelihood with the new solution\n",
    "    theta_pol, delta_HWP, offset_HWP, delta_derot, offset_derot, delta_opts, epsilon_opts, rot_opts, delta_FLC, rot_FLC = soln.x\n",
    "    delta_m3, epsilon_m3, offset_m3, em_gain = fixed_params\n",
    "    all_params = [delta_m3, epsilon_m3, offset_m3, delta_HWP, offset_HWP, \n",
    "        delta_derot, offset_derot, delta_opts, epsilon_opts, rot_opts, delta_FLC, \n",
    "        rot_FLC, em_gain]\n",
    "    post_likelihood = np.sum((instrument_matrices.internal_calibration_mueller_matrix(theta_pol, model, all_params, HWP_angs, IMR_angs) - reshaped_data) / reshaped_stds ** 2)\n",
    "\n",
    "    print(\"Iteration #\" + str(counter) + \": \" + str(post_likelihood))\n",
    "\n",
    "    print(\"MAXIMUM LIKELIHOOD ESTIMATES\")\n",
    "    print(\"\")\n",
    "    print(\"theta_pol (degrees): \" + str(theta_pol))\n",
    "    print(\"delta_HWP (waves): \" + str(delta_HWP))\n",
    "    print(\"offset_HWP (degrees): \" + str(offset_HWP))\n",
    "    print(\"delta_derot (waves): \" + str(delta_derot))\n",
    "    print(\"offset_derot (degrees): \" + str(offset_derot))\n",
    "    print(\"delta_opts (waves): \" + str(delta_opts))\n",
    "    print(\"epsilon_opts: \" + str(epsilon_opts))\n",
    "    print(\"rot_opts (degrees): \" + str(rot_opts))\n",
    "    print(\"delta_FLC (waves): \" + str(delta_FLC))\n",
    "    print(\"rot_FLC (degrees): \" + str(rot_FLC))\n",
    "\n",
    "    reshaped_data = general.reshape_and_flatten(data)\n",
    "    reshaped_stds = general.reshape_and_flatten(stds)\n",
    "\n",
    "    model_1 = instrument_matrices.internal_calibration_mueller_matrix(theta_pol, model, all_params, HWP_angs, IMR_angs)\n",
    "    residuals_1 = model_1 - reshaped_data\n",
    "\n",
    "    # data_plotting.plot_single_model_and_residuals(angles, angles, model_1, data, \n",
    "    #     residuals_1, stds, log_f, wavelength, fig_dimensions = (30, 20))\n",
    "\n",
    "    # Reset initial guess\n",
    "    initial_guess = soln.x\n",
    "\n",
    "    print(\"Post Likelihood: \" + str(post_likelihood))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
